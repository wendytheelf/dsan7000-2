# llm:
#   provider: ollama
#   base_url: "http://localhost:11434/v1"
#   api_key: "ollama"
#   model: "llama3:8b"
#   temperature: 0.2
#   max_tokens: 512


# model:
#   model: "qwen2.5:7b-instruct"  
#   max_tokens: 512
#   temperature: 0.0      # 0.0–0.2

confidence_threshold: 0.75
retrieval_top_n: 5
top_n_docs: 5
auto_accept_threshold: 0.8
review_threshold: 0.5
max_tokens: 1024        # 這兩個是 pipeline 其他地方用的全域參數，不影響 Ollama 直接呼叫
temperature: 0.0
units:
  power: "kW"
  flow: "L/s"
  length: "m"
  area: "m²"
  temperature: "°C"
output_format: "csv"


